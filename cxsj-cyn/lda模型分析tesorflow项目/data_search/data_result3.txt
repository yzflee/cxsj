System information Have written custom opposed using stock script provided TensorFlow No OS Platform Distribution Linux Ubuntu Mobile iPhone Pixel Samsung Galaxy happens mobile TensorFlow installed binary TensorFlow command rc Bazel compiling GCC Compiler compiling CUDA cuDNN GPU memory Describe current behavior Previously running CVAE tutorial beta tutorials generative cvae sample function eps None Which worked fine however rc throws error comparing None works following eps None Describe expected behavior Comparing something None None behavior Code reproduce See beta tutorials generative cvae change sample function described em Please make sure bug As per GitHub Policy blob master ISSUES md address bugs performance issues feature requests build installation issues GitHub tag bug template em System information Have written custom Nop assembled statements OS Platform Distribution Linux Ubuntu MacOS Mobile iPhone Pixel Samsung Galaxy happens mobile Samsung S TensorFlow installed binary binary TensorFlow command nightly dev Describe current behavior trying Keras MobileNet float precision Gpu Inference But running tasks encountered following Error Caused java lang IllegalArgumentException Internal error Failed apply delegate Next operations supported GPU delegate MEAN Operation supported First operations GPU remaining CPU lite kernels conv bias Node number CONV D failed prepare lite kernels conv bias Node number CONV D failed prepare Describe expected behavior Code reproduce script conversion keras keras keras applications mobilenet MobileNet shape None alpha depth multiplier dropout top True weights imagenet None pooling None classes converter lite TFLiteConverter keras mobilenet h converter optimizations lite Optimize DEFAULT converter target spec supported types lite constants FLOAT tflite converter open mobilenet tflite wb write tflite Android called tfliteOptions setAllowFp PrecisionForFp true Other info logs Include logs helpful diagnose problem If including tracebacks please full traceback Large logs attached Successfully tested Hello World Arduino MKR WiFi fading LED effect System information Have written custom opposed using stock script provided TensorFlow Yes network D convultions batch normalization image OS Platform Distribution Linux Ubuntu Ubuntu Mobile iPhone Pixel Samsung Galaxy happens mobile TensorFlow installed binary TensorFlow command TF Bazel compiling A GCC Compiler compiling CUDA cuDNN GPU memory T GB Describe current behavior trying optimize custom comprised D convolutions batch normalizations done image The entire network fixed dimensions using nightly docker TF image perform TF TRT tried create TRT using following functions create saved saved dir saved dir precision batch saved TRT saved converter TrtGraphConverter saved dir str saved dir max batch batch precision mode precision converter converter save saved dir str saved dir create frozen graph graph nodes precision graph path None workspace batch frozen graph TRT frozen graph converter TrtGraphConverter graph graph nodes blacklist nodes max batch batch max workspace bytes workspace precision mode precision graph converter graph path None write graph graph graph path graph In cases TF TRT X slower vs inference The results regardless graph memory load TF TRT saved Here respective TRT compiler segment segment There ops different types graph converted TensorRT Identity NoOp Placeholder For information see docs nvidia deeplearning frameworks guide index html supported ops compiler graph Number TensorRT candidate segments stream executor platform default dso loader Successfully opened dynamic library libnvinfer stream executor platform default dso loader Successfully opened dynamic library libnvinfer plugin compiler graph TensorRT TRTEngineOp added segment consisting nodes succeeded W compiler optimization pass TensorRTOptimizer probably called funcdef This optimizer must NOT called function objects grappler optimizers meta optimizer Optimization results grappler item graph grappler optimizers meta optimizer constant folding Graph nodes edges time grappler optimizers meta optimizer layout Graph nodes edges time grappler optimizers meta optimizer constant folding Graph nodes edges time grappler optimizers meta optimizer TensorRTOptimizer Graph nodes edges time grappler optimizers meta optimizer constant folding Graph nodes edges time grappler optimizers meta optimizer Optimization results grappler item TRTEngineOp native segment grappler optimizers meta optimizer constant folding Graph nodes edges time grappler optimizers meta optimizer layout Graph nodes edges time grappler optimizers meta optimizer constant folding Graph nodes edges time grappler optimizers meta optimizer TensorRTOptimizer Graph nodes edges time grappler optimizers meta optimizer constant folding Graph nodes edges time stream executor cuda cuda gpu executor successful NUMA read SysFS negative value must Since TRT nodes one TRT Engine make sense via UFF Would get speed improvement Or bug latest TF docker Thanks Describe expected behavior Code reproduce Provide reproducible test case bare minimum necessary generate problem Other info logs Include logs helpful diagnose problem If including tracebacks please full traceback Large logs attached 