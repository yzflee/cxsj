This PR allows add dependency Go API using rules go bazel trying fine tune inceptionv using slim library tried read proper documentation figured things able fine tune save checkpoint However unable understand certain things writing Here steps followed created record training fine reading using contrib slim nets nets contrib slim slim matplotlib pyplot plt numpy np get labels path home sfarkya nvidia challenge datasets detrac train tfrecords Training setting num epochs initial learning rate learning rate decay factor num epochs decay num classes load checkpoint path home sfarkya nvidia challenge datasets detrac inception ckpt log directory log dir home sfarkya nvidia challenge datasets detrac fine tuned Session sess feature { train image FixedLenFeature string train label FixedLenFeature } Create list filenames pass queue filename queue train string producer path num epochs Define reader read next record reader TFRecordReader serialized reader read filename queue Decode record read reader features parse single serialized features feature Convert image string back numbers image decode raw features train image float Cast label label cast features train label Reshape image original shape image reshape image Creates batches randomly shuffling tensors images labels train shuffle batch image label batch capacity num threads min dequeue url Now finetuning using slim init op group global variables initializer local variables initializer sess init op Create coordinator QueueRunner objects coord train Coordinator threads train start queue runners coord coord load load inception slim library using inception inputL placeholder float img lbl sess images labels one hot labels slim one hot encoding lbl num classes slim arg scope slim nets inception inception arg scope logits inceptionv nets inception inception inputs img num classes training True dropout keep prob Restore convolutional layers variables restore slim get variables restore exclude InceptionV Logits InceptionV AuxLogits init slim assign checkpoint path variables restore loss function loss losses softmax cross entropy onehot labels one hot labels logits logits total loss losses get total loss train operation train op slim learning create train op total loss loss optimizer train AdamOptimizer learning rate print Im Start training slim learning train train op log dir init init save interval secs number steps Now questions quite unable figure Once reaches slim learning train see anything printing however training see log Now How give number epochs Right running step step step batch How make sure train shuffle batch repeating images training whole dataset How print loss values training If create validation switch betweem training validation Thanks help System information Have written custom opposed using stock script provided TensorFlow yes OS Platform Distribution Linux Ubuntu Linux amd SMP Debian deb u GNU Linux TensorFlow installed binary binary TensorFlow command rc Bazel compiling A GCC Compiler compiling A CUDA cuDNN A GPU memory Google Cloud TPU running TF Exact command reproduce See Describe problem Using Keras LSTM layer processing Google Cloud TPU results ValueError lstm unrolled The layer behaves expected TPUEstimator configured CPU lstm loop unrolled either TPU CPU Error raised TPU failure case ValueError Cannot create gradient accumulator TPUReplicate loop lstm Identity inside XLA loop maximum iterations passed loop call TPUReplicate loop lstm context Source logs Test minimal keras lstm test Test Keras LSTM TPU Based tpu blob master models official resnet future absolute future division future print function absl flags absl logging logging pylint disable unused numpy np contrib tpu tpu tpu config contrib tpu tpu tpu estimator contrib tpu tpu tpu optimizer Define flags system FLAGS flags FLAGS flags DEFINE bool tpu True help Use TPU execute training evaluation If tpu false whatever devices available TensorFlow default CPU GPU flags DEFINE string tpu name default None help Name Cloud TPU Cluster Resolvers flags DEFINE string dir default None help The directory training evaluation summaries stored flags DEFINE bool unroll lstm default False help Unrolls Keras LSTM True main unused argv Get TPU GRPC URL needed FLAGS tpu tpu cluster resolver contrib cluster resolver TPUClusterResolver FLAGS tpu name tpu grpc url tpu cluster resolver get master else tpu grpc url None Set configuration TPU config tpu config RunConfig master tpu grpc url dir FLAGS dir Create TPUEstimator test estimator tpu estimator TPUEstimator tpu FLAGS tpu test config config train batch Train estimator steps test estimator train max steps params Generate dataset correct shape np rand astype np float label np rand astype np float Repeat batch rand dataset Dataset slices label repeat rand dataset rand dataset apply contrib batch drop remainder params batch Make TPUEstimator train step rand dataset rand dataset make one shot iterator get next rand dataset test features labels mode params The Keras LSTM layer causes error unless unrolled predictions keras layers LSTM unroll FLAGS unroll lstm features Create ops TPUEstimatorSpec loss losses mean squared error labels labels predictions predictions optimizer train GradientDescentOptimizer FLAGS tpu optimizer tpu optimizer CrossShardOptimizer optimizer global step train get global step train op optimizer minimize loss global step Return TPUEstimatorSpec tpu estimator TPUEstimatorSpec mode mode loss loss train op train op name main Do thing print main wrapper logging verbosity logging INFO app Error TPU unroll loop False sanitized log $ keras lstm test dir gs bucket keras lstm test tpu name tpu name unroll lstm False WARNING Logging flag parsing goes stderr W logging From local dist packages contrib learn learn datasets base retry contrib learn learn datasets base deprecated removed future Instructions updating Use retry module similar alternatives main wrapper W init cache unavailable using oauth client Traceback recent call last local dist packages googleapiclient discovery cache init autodetect cache local dist packages googleapiclient discovery cache cache module cache unavailable using oauth client ImportError cache unavailable using oauth client platform cpu feature guard Your CPU supports instructions TensorFlow binary compiled AVX FMA distributed runtime rpc grpc channel Initialize GrpcChannelCache job local { localhost } distributed runtime rpc grpc Started target grpc localhost W logging Estimator function test f eca f includes params argument params passed Estimator logging Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max seed None task worker global id cluster chief True cluster spec training ClusterSpec object f eca cluster None dir gs bucket keras lstm test num worker replicas task id log step count steps master u grpc save checkpoints steps None keep checkpoint every hours evaluation master u grpc service None save summary steps num ps replicas } logging Querying master grpc TPU system metadata W distributed runtime rpc grpc session GrpcSession ListDevices initialize session empty graph defaults session yet created logging Found TPU system logging Num TPU Cores logging Num TPU Workers logging Num TPU Cores Per Worker logging Available Devices DeviceAttributes job tpu worker replica task CPU CPU DeviceAttributes job tpu worker replica task XLA CPU XLA CPU DeviceAttributes job tpu worker replica task XLA GPU XLA GPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU SYSTEM TPU SYSTEM logging Calling Traceback recent call last keras lstm test module app local dist packages platform app sys exit main argv keras lstm test main test estimator train max steps local dist packages estimator estimator train loss self train hooks saving listeners local dist packages estimator estimator train features labels ModeKeys TRAIN self config local dist packages estimator estimator call results self features features kwargs local dist packages contrib tpu tpu tpu estimator train tpu system ctx wrapper dequeue local dist packages contrib tpu tpu tpu estimator train tpu system assignment ctx assignment local dist packages contrib tpu tpu tpu shard name name local dist packages contrib tpu tpu tpu replicate outputs computation computation inputs local dist packages contrib tpu tpu tpu estimator multi tpu train steps single shard name loop local dist packages contrib tpu tpu training loop repeat cond body wrapper inputs inputs infeed queue infeed queue name name local dist packages contrib tpu tpu training loop loop name name local dist packages ops control flow ops loop result loop context BuildLoop cond body loop vars shape invariants local dist packages ops control flow ops BuildLoop pred body original loop vars loop vars shape invariants local dist packages ops control flow ops BuildLoop body result body packed vars body local dist packages contrib tpu tpu training loop body wrapper outputs body inputs dequeue ops local dist packages contrib tpu tpu training loop body wrapper list body args local dist packages contrib tpu tpu tpu estimator train step self call features labels local dist packages contrib tpu tpu tpu estimator call estimator spec self features features kwargs keras lstm test test train op optimizer minimize loss global step local dist packages training optimizer minimize grad loss grad loss local dist packages contrib tpu tpu tpu optimizer compute gradients self opt compute gradients loss var list var list kwargs local dist packages training optimizer compute gradients colocate gradients ops colocate gradients ops local dist packages ops gradients impl gradients gate gradients aggregation method stop gradients local dist packages ops gradients impl GradientsHelper lambda grad op grads local dist packages ops gradients impl MaybeCompile grad Exit early local dist packages ops gradients impl lambda lambda grad op grads local dist packages ops array grad TensorArrayWriteGrad grad read index local dist packages ops array ops read self implementation read index name name local dist packages ops array ops read name name local dist packages ops gen flow ops array read dtype dtype name name local dist packages framework op library apply op helper op op local dist packages framework ops create op op op local dist packages framework ops init self control flow post processing local dist packages framework ops control flow post processing self control flow context AddOp self local dist packages ops control flow ops AddOp self AddOpInternal op local dist packages ops control flow ops AddOpInternal real self AddValue local dist packages ops control flow ops AddValue real val grad ctxt grad state GetRealValue val local dist packages ops control flow ops GetRealValue history value cur grad state AddForwardAccumulator cur value local dist packages ops control flow ops AddForwardAccumulator value self forward context local dist packages ops control flow ops GetMaxSizeFromNestedMaximumIterations loop call % % value name ctxt name ValueError Cannot create gradient accumulator TPUReplicate loop lstm Identity inside XLA loop maximum iterations passed loop call TPUReplicate loop lstm context Working expected TPU unroll lstm True $ keras lstm test dir gs bucket keras lstm test tpu name tpu name unroll lstm True WARNING Logging flag parsing goes stderr W logging From local dist packages contrib learn learn datasets base retry contrib learn learn datasets base deprecated removed future Instructions updating Use retry module similar alternatives main wrapper W init cache unavailable using oauth client Traceback recent call last local dist packages googleapiclient discovery cache init autodetect cache local dist packages googleapiclient discovery cache cache module cache unavailable using oauth client ImportError cache unavailable using oauth client platform cpu feature guard Your CPU supports instructions TensorFlow binary compiled AVX FMA distributed runtime rpc grpc channel Initialize GrpcChannelCache job local { localhost } distributed runtime rpc grpc Started target grpc localhost W logging Estimator function test ffb dff includes params argument params passed Estimator logging Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max seed None task worker global id cluster chief True cluster spec training ClusterSpec object ffb dfdcd cluster None dir gs bucket keras lstm test num worker replicas task id log step count steps master u grpc save checkpoints steps None keep checkpoint every hours evaluation master u grpc service None save summary steps num ps replicas } logging Querying master grpc TPU system metadata W distributed runtime rpc grpc session GrpcSession ListDevices initialize session empty graph defaults session yet created logging Found TPU system logging Num TPU Cores logging Num TPU Workers logging Num TPU Cores Per Worker logging Available Devices DeviceAttributes job tpu worker replica task CPU CPU DeviceAttributes job tpu worker replica task XLA CPU XLA CPU DeviceAttributes job tpu worker replica task XLA GPU XLA GPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU TPU DeviceAttributes job tpu worker replica task TPU SYSTEM TPU SYSTEM logging Calling logging Done calling logging Create CheckpointSaverHook logging TPU job name tpu worker logging Graph finalized logging Running local init op logging Done running local init op logging Init TPU system logging Start infeed thread controller logging Starting infeed thread controller logging Start outfeed thread controller logging Starting outfeed thread controller logging Enqueue next batch es infeed logging Dequeue next batch es outfeed logging Saving checkpoints gs bucket keras lstm test ckpt logging loss step logging loss step logging Enqueue next batch es infeed logging Dequeue next batch es outfeed logging Enqueue next batch es infeed logging Dequeue next batch es outfeed logging Enqueue next batch es infeed logging Dequeue next batch es outfeed logging Enqueue next batch es infeed logging Dequeue next batch es outfeed logging Saving checkpoints gs bucket keras lstm test ckpt logging Stop infeed thread controller logging Shutting InfeedController thread logging InfeedController received shutdown signal stopping logging Infeed thread finished shutting logging Stop thread controller logging Shutting OutfeedController thread logging OutfeedController received shutdown signal stopping logging Outfeed thread finished shutting logging Shutdown TPU system logging Loss final step Working expected unroll loop either True False CPU $ keras lstm test dir gs bucket keras lstm test tpu False unroll lstm False WARNING Logging flag parsing goes stderr W logging From local dist packages contrib learn learn datasets base retry contrib learn learn datasets base deprecated removed future Instructions updating Use retry module similar alternatives main wrapper W logging Estimator function test f ace f includes params argument params passed Estimator logging Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max seed None task worker global id cluster chief True cluster spec training ClusterSpec object f ace cluster None dir gs bucket keras lstm test num worker replicas task id log step count steps master save checkpoints steps None keep checkpoint every hours evaluation master service None save summary steps num ps replicas } logging Calling logging Running train CPU logging Done calling logging Create CheckpointSaverHook logging Graph finalized platform cpu feature guard Your CPU supports instructions TensorFlow binary compiled AVX FMA logging Running local init op logging Done running local init op logging Saving checkpoints gs bucket keras lstm test ckpt logging loss step logging Saving checkpoints gs bucket keras lstm test ckpt logging Loss final step $ keras lstm test dir gs bucket keras lstm test tpu False unroll lstm True WARNING Logging flag parsing goes stderr W logging From local dist packages contrib learn learn datasets base retry contrib learn learn datasets base deprecated removed future Instructions updating Use retry module similar alternatives main wrapper W logging Estimator function test fb includes params argument params passed Estimator logging Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max seed None task worker global id cluster chief True cluster spec training ClusterSpec object fb cluster None dir gs bucket keras lstm test num worker replicas task id log step count steps master save checkpoints steps None keep checkpoint every hours evaluation master service None save summary steps num ps replicas } logging Calling logging Running train CPU logging Done calling logging Create CheckpointSaverHook logging Graph finalized platform cpu feature guard Your CPU supports instructions TensorFlow binary compiled AVX FMA logging Running local init op logging Done running local init op logging Saving checkpoints gs bucket keras lstm test ckpt logging loss step logging Saving checkpoints gs bucket keras lstm test ckpt logging Loss final step 