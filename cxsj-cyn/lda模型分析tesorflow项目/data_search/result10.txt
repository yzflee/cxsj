This PR allows add dependency Go API using rules go bazel trying fine tune inceptionv using slim library tried read source proper documentation figured things able fine tune save checkpoint However unable understand certain things writing Here steps followed created record training fine reading using contrib slim nets nets contrib slim slim matplotlib pyplot plt numpy np get labels path home sfarkya nvidia challenge datasets detrac train tfrecords Training setting num epochs initial learning rate learning rate decay factor num epochs decay num classes load checkpoint path home sfarkya nvidia challenge datasets detrac inception ckpt log directory log dir home sfarkya nvidia challenge datasets detrac fine tuned Session sess feature { train image FixedLenFeature string train label FixedLenFeature } Create list filenames pass queue filename queue train string producer path num epochs Define reader read next record reader TFRecordReader serialized reader read filename queue Decode record read reader features parse single serialized features feature Convert image string back numbers image decode raw features train image float Cast label label cast features train label Reshape image original shape image reshape image Creates batches randomly shuffling tensors images labels train shuffle batch image label batch capacity num threads min dequeue url Now finetuning using slim init op group global variables initializer local variables initializer sess init op Create coordinator QueueRunner objects coord train Coordinator threads train start queue runners coord coord load load inception slim library using inception inputL placeholder float img lbl sess images labels one hot labels slim one hot encoding lbl num classes slim arg scope slim nets inception inception arg scope logits inceptionv nets inception inception inputs img num classes training True dropout keep prob Restore convolutional layers variables restore slim get variables restore exclude InceptionV Logits InceptionV AuxLogits init fn slim assign checkpoint fn path variables restore loss function loss losses softmax cross entropy onehot labels one hot labels logits logits total loss losses get total loss train operation train op slim learning create train op total loss loss optimizer train AdamOptimizer learning rate print Im Start training slim learning train train op log dir init fn init fn save interval secs number steps Now questions quite unable figure Once reaches slim learning train see anything printing however training see log Now How give number epochs Right running step step step batch How make sure train shuffle batch repeating images training whole dataset How print loss values training If create validation switch betweem training validation Thanks help System information Have written custom opposed using stock script provided TensorFlow yes OS Platform Distribution Linux Ubuntu Linux amd SMP Debian deb u GNU Linux TensorFlow installed source binary binary TensorFlow command rc Bazel compiling source N A GCC Compiler compiling source N A CUDA cuDNN N A GPU memory Google Cloud TPU running TF Exact command reproduce See Describe problem Using Keras LSTM layer processing Google Cloud TPU results ValueError lstm unrolled The layer behaves expected TPUEstimator configured CPU lstm loop unrolled either TPU CPU Error raised TPU failure case ValueError Cannot create gradient accumulator TPUReplicate loop lstm Identity inside XLA loop maximum iterations passed loop call TPUReplicate loop lstm context Source logs Test minimal keras lstm test Test Keras LSTM TPU Based tpu blob master models official resnet future absolute future division future print function absl flags absl logging logging pylint disable unused numpy np contrib tpu tpu tpu config contrib tpu tpu tpu estimator contrib tpu tpu tpu optimizer Define flags system FLAGS flags FLAGS flags DEFINE bool tpu True help Use TPU execute training evaluation If tpu false whatever devices available TensorFlow default CPU GPU flags DEFINE string tpu name default None help Name Cloud TPU Cluster Resolvers flags DEFINE string dir default None help The directory training evaluation summaries stored flags DEFINE bool unroll lstm default False help Unrolls Keras LSTM True main unused argv Get TPU GRPC URL needed FLAGS tpu tpu cluster resolver contrib cluster resolver TPUClusterResolver FLAGS tpu name tpu grpc url tpu cluster resolver get master else tpu grpc url None Set configuration TPU config tpu config RunConfig master tpu grpc url dir FLAGS dir Create TPUEstimator test estimator tpu estimator TPUEstimator tpu FLAGS tpu fn test fn config config train batch Train estimator steps test estimator train fn max steps fn params Generate random dataset correct shape np random rand astype np float label np random rand astype np float Repeat batch rand dataset Dataset slices label repeat rand dataset rand dataset apply contrib batch drop remainder params batch Make fn TPUEstimator train step rand dataset fn rand dataset make one shot iterator get next return rand dataset fn test fn features labels mode params The Keras LSTM layer causes error unless unrolled predictions keras layers LSTM unroll FLAGS unroll lstm features Create ops TPUEstimatorSpec loss losses mean squared error labels labels predictions predictions optimizer train GradientDescentOptimizer FLAGS tpu optimizer tpu optimizer CrossShardOptimizer optimizer global step train get global step train op optimizer minimize loss global step Return TPUEstimatorSpec return tpu estimator TPUEstimatorSpec mode mode loss loss train op train op name main Do thing print main wrapper logging verbosity logging INFO app Error TPU unroll loop False sanitized log $ keras lstm test dir gs bucket keras lstm test tpu name tpu name unroll lstm False WARNING Logging flag parsing goes stderr W logging From local dist packages contrib learn learn datasets base retry contrib learn learn datasets base deprecated removed future Instructions updating Use retry module similar alternatives main wrapper W init cache unavailable using oauth client Traceback recent call last File local dist packages googleapiclient discovery cache init autodetect cache File local dist packages googleapiclient discovery cache cache module cache unavailable using oauth client ImportError cache unavailable using oauth client cpu feature guard Your CPU supports instructions TensorFlow binary compiled AVX FMA distributed runtime rpc grpc channel Initialize GrpcChannelCache job local { localhost } distributed runtime rpc grpc Started target grpc localhost W logging Estimator fn function test fn f eca f includes params argument params passed Estimator logging Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max random seed None task type worker global id cluster chief True cluster spec training ClusterSpec object f eca cluster None dir gs bucket keras lstm test num worker replicas task id log step count steps master u grpc save checkpoints steps None keep checkpoint every n hours evaluation master u grpc service None save summary steps num ps replicas } logging Querying master grpc TPU system metadata W distributed runtime rpc grpc session GrpcSession ListDevices initialize session empty graph defaults session yet created logging Found TPU system logging Num TPU Cores logging Num TPU Workers logging Num TPU Cores Per Worker logging Available Devices DeviceAttributes job tpu worker replica task device CPU CPU DeviceAttributes job tpu worker replica task device XLA CPU XLA CPU DeviceAttributes job tpu worker replica task device XLA GPU XLA GPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU SYSTEM TPU SYSTEM logging Calling fn Traceback recent call last File keras lstm test module app File local dist packages app sys exit main argv File keras lstm test main test estimator train fn max steps File local dist packages estimator estimator train loss self train fn hooks saving listeners File local dist packages estimator estimator train features labels fn ModeKeys TRAIN self config File local dist packages estimator estimator call fn fn results self fn features features kwargs File local dist packages contrib tpu tpu tpu estimator fn train tpu system ctx fn wrapper dequeue fn File local dist packages contrib tpu tpu tpu estimator train tpu system device assignment ctx device assignment File local dist packages contrib tpu tpu tpu shard name name File local dist packages contrib tpu tpu tpu replicate outputs computation computation inputs File local dist packages contrib tpu tpu tpu estimator multi tpu train steps single shard name loop File local dist packages contrib tpu tpu training loop repeat cond body wrapper inputs inputs infeed queue infeed queue name name File local dist packages contrib tpu tpu training loop loop name name File local dist packages ops control flow ops loop result loop context BuildLoop cond body loop vars shape invariants File local dist packages ops control flow ops BuildLoop pred body original loop vars loop vars shape invariants File local dist packages ops control flow ops BuildLoop body result body packed vars body File local dist packages contrib tpu tpu training loop body wrapper outputs body inputs dequeue ops File local dist packages contrib tpu tpu training loop body wrapper return list body args File local dist packages contrib tpu tpu tpu estimator train step self call fn features labels File local dist packages contrib tpu tpu tpu estimator call fn estimator spec self fn features features kwargs File keras lstm test test fn train op optimizer minimize loss global step File local dist packages training optimizer minimize grad loss grad loss File local dist packages contrib tpu tpu tpu optimizer compute gradients return self opt compute gradients loss var list var list kwargs File local dist packages training optimizer compute gradients colocate gradients ops colocate gradients ops File local dist packages ops gradients impl gradients gate gradients aggregation method stop gradients File local dist packages ops gradients impl GradientsHelper lambda grad fn op grads File local dist packages ops gradients impl MaybeCompile return grad fn Exit early File local dist packages ops gradients impl lambda lambda grad fn op grads File local dist packages ops array grad TensorArrayWriteGrad grad read index File local dist packages ops array ops read return self implementation read index name name File local dist packages ops array ops read name name File local dist packages ops gen flow ops array read dtype dtype name name File local dist packages framework op library apply op helper op op File local dist packages framework ops create op op op File local dist packages framework ops init self control flow post processing File local dist packages framework ops control flow post processing self control flow context AddOp self File local dist packages ops control flow ops AddOp self AddOpInternal op File local dist packages ops control flow ops AddOpInternal real self AddValue File local dist packages ops control flow ops AddValue real val grad ctxt grad state GetRealValue val File local dist packages ops control flow ops GetRealValue history value cur grad state AddForwardAccumulator cur value File local dist packages ops control flow ops AddForwardAccumulator value self forward context File local dist packages ops control flow ops GetMaxSizeFromNestedMaximumIterations loop call % % value name ctxt name ValueError Cannot create gradient accumulator TPUReplicate loop lstm Identity inside XLA loop maximum iterations passed loop call TPUReplicate loop lstm context Working expected TPU unroll lstm True $ keras lstm test dir gs bucket keras lstm test tpu name tpu name unroll lstm True WARNING Logging flag parsing goes stderr W logging From local dist packages contrib learn learn datasets base retry contrib learn learn datasets base deprecated removed future Instructions updating Use retry module similar alternatives main wrapper W init cache unavailable using oauth client Traceback recent call last File local dist packages googleapiclient discovery cache init autodetect cache File local dist packages googleapiclient discovery cache cache module cache unavailable using oauth client ImportError cache unavailable using oauth client cpu feature guard Your CPU supports instructions TensorFlow binary compiled AVX FMA distributed runtime rpc grpc channel Initialize GrpcChannelCache job local { localhost } distributed runtime rpc grpc Started target grpc localhost W logging Estimator fn function test fn ffb dff includes params argument params passed Estimator logging Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max random seed None task type worker global id cluster chief True cluster spec training ClusterSpec object ffb dfdcd cluster None dir gs bucket keras lstm test num worker replicas task id log step count steps master u grpc save checkpoints steps None keep checkpoint every n hours evaluation master u grpc service None save summary steps num ps replicas } logging Querying master grpc TPU system metadata W distributed runtime rpc grpc session GrpcSession ListDevices initialize session empty graph defaults session yet created logging Found TPU system logging Num TPU Cores logging Num TPU Workers logging Num TPU Cores Per Worker logging Available Devices DeviceAttributes job tpu worker replica task device CPU CPU DeviceAttributes job tpu worker replica task device XLA CPU XLA CPU DeviceAttributes job tpu worker replica task device XLA GPU XLA GPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU SYSTEM TPU SYSTEM logging Calling fn logging Done calling fn logging Create CheckpointSaverHook logging TPU job name tpu worker logging Graph finalized logging Running local init op logging Done running local init op logging Init TPU system logging Start infeed thread controller logging Starting infeed thread controller logging Start outfeed thread controller logging Starting outfeed thread controller logging Enqueue next batch es infeed logging Dequeue next batch es outfeed logging Saving checkpoints gs bucket keras lstm test ckpt logging loss step logging loss step logging Enqueue next batch es infeed logging Dequeue next batch es outfeed logging Enqueue next batch es infeed logging Dequeue next batch es outfeed logging Enqueue next batch es infeed logging Dequeue next batch es outfeed logging Enqueue next batch es infeed logging Dequeue next batch es outfeed logging Saving checkpoints gs bucket keras lstm test ckpt logging Stop infeed thread controller logging Shutting InfeedController thread logging InfeedController received shutdown signal stopping logging Infeed thread finished shutting logging Stop thread controller logging Shutting OutfeedController thread logging OutfeedController received shutdown signal stopping logging Outfeed thread finished shutting logging Shutdown TPU system logging Loss final step Working expected unroll loop either True False CPU $ keras lstm test dir gs bucket keras lstm test tpu False unroll lstm False WARNING Logging flag parsing goes stderr W logging From local dist packages contrib learn learn datasets base retry contrib learn learn datasets base deprecated removed future Instructions updating Use retry module similar alternatives main wrapper W logging Estimator fn function test fn f ace f includes params argument params passed Estimator logging Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max random seed None task type worker global id cluster chief True cluster spec training ClusterSpec object f ace cluster None dir gs bucket keras lstm test num worker replicas task id log step count steps master save checkpoints steps None keep checkpoint every n hours evaluation master service None save summary steps num ps replicas } logging Calling fn logging Running train CPU logging Done calling fn logging Create CheckpointSaverHook logging Graph finalized cpu feature guard Your CPU supports instructions TensorFlow binary compiled AVX FMA logging Running local init op logging Done running local init op logging Saving checkpoints gs bucket keras lstm test ckpt logging loss step logging Saving checkpoints gs bucket keras lstm test ckpt logging Loss final step $ keras lstm test dir gs bucket keras lstm test tpu False unroll lstm True WARNING Logging flag parsing goes stderr W logging From local dist packages contrib learn learn datasets base retry contrib learn learn datasets base deprecated removed future Instructions updating Use retry module similar alternatives main wrapper W logging Estimator fn function test fn fb includes params argument params passed Estimator logging Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max random seed None task type worker global id cluster chief True cluster spec training ClusterSpec object fb cluster None dir gs bucket keras lstm test num worker replicas task id log step count steps master save checkpoints steps None keep checkpoint every n hours evaluation master service None save summary steps num ps replicas } logging Calling fn logging Running train CPU logging Done calling fn logging Create CheckpointSaverHook logging Graph finalized cpu feature guard Your CPU supports instructions TensorFlow binary compiled AVX FMA logging Running local init op logging Done running local init op logging Saving checkpoints gs bucket keras lstm test ckpt logging loss step logging Saving checkpoints gs bucket keras lstm test ckpt logging Loss final step 