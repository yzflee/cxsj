System information Have I written custom code opposed using stock example script provided TensorFlow No OS Platform Distribution e g Linux Ubuntu Mobile device e g iPhone Pixel Samsung Galaxy issue happens mobile device TensorFlow installed source binary TensorFlow version use command rc Python version Bazel version compiling source GCC Compiler version compiling source CUDA cuDNN version GPU model memory Describe current behavior Previously running code CVAE tutorial https www tensorflow org beta tutorials generative cvae I used sample function eps None Which worked fine however rc throws error comparing tensor None works following used eps None Describe expected behavior Comparing something None None behavior Code reproduce issue See https www tensorflow org beta tutorials generative cvae change sample function described em Please make sure bug As per GitHub Policy https github com tensorflow tensorflow blob master ISSUES md address code doc bugs performance issues feature requests build installation issues GitHub tag bug template em System information Have I written custom code Nop assembled statements OS Platform Distribution e g Linux Ubuntu MacOS Mobile device e g iPhone Pixel Samsung Galaxy issue happens mobile device Samsung S TensorFlow installed source binary binary TensorFlow version use command tf nightly dev Python version Describe current behavior I trying convert Keras MobileNet model float precision Gpu Inference But running tasks I encountered following Error Caused java lang IllegalArgumentException Internal error Failed apply delegate Next operations supported GPU delegate MEAN Operation supported First operations run GPU remaining CPU tensorflow lite kernels conv cc bias type input type Node number CONV D failed prepare tensorflow lite kernels conv cc bias type input type Node number CONV D failed prepare Describe expected behavior Code reproduce issue Python script conversion python import tensorflow tf import tensorflow keras keras model keras applications mobilenet MobileNet input shape None alpha depth multiplier dropout e include top True weights imagenet input tensor None pooling None classes converter tf lite TFLiteConverter keras model file mobilenet h converter optimizations tf lite Optimize DEFAULT converter target spec supported types tf lite constants FLOAT tflite model converter convert open mobilenet tflite wb write tflite model Android code I called tfliteOptions setAllowFp PrecisionForFp true Other info logs Include logs source code would helpful diagnose problem If including tracebacks please include full traceback Large logs files attached Successfully tested Hello World example Arduino MKR WiFi fading LED effect System information Have I written custom code opposed using stock example script provided TensorFlow Yes I network D convultions + batch normalization image OS Platform Distribution e g Linux Ubuntu Ubuntu Mobile device e g iPhone Pixel Samsung Galaxy issue happens mobile device TensorFlow installed source binary TensorFlow version use command TF Python version Bazel version compiling source N A GCC Compiler version compiling source CUDA cuDNN version GPU model memory T GB Describe current behavior I trying optimize custom model comprised D convolutions batch normalizations done image The entire network fixed dimensions I using nightly docker TF image perform TF TRT I tried create TRT model using following functions def create trt saved model saved model dir output saved model dir precision batch size convert saved model TRT saved model converter trt TrtGraphConverter input saved model dir str saved model dir max batch size batch size precision mode precision converter convert converter save output saved model dir str output saved model dir def create trt frozen graph graph def output nodes precision output graph path None workspace size batch size convert frozen graph TRT frozen graph converter trt TrtGraphConverter input graph def graph def nodes blacklist output nodes max batch size batch size max workspace size bytes workspace size precision mode precision trt graph def converter convert output graph path None write graph file trt graph def output graph path return trt graph def In cases TF TRT model X slower ms vs ms inference The results regardless I use graph def memory load TF TRT saved model Here respective TRT output I tensorflow compiler tf tensorrt segment segment cc There ops different types graph converted TensorRT Identity NoOp Placeholder For information see https docs nvidia com deeplearning frameworks tf trt user guide index html supported ops I tensorflow compiler tf tensorrt convert convert graph cc Number TensorRT candidate segments I tensorflow stream executor platform default dso loader cc Successfully opened dynamic library libnvinfer I tensorflow stream executor platform default dso loader cc Successfully opened dynamic library libnvinfer plugin I tensorflow compiler tf tensorrt convert convert graph cc TensorRT node TRTEngineOp added segment consisting nodes succeeded W tensorflow compiler tf tensorrt convert trt optimization pass cc TensorRTOptimizer probably called funcdef This optimizer must NOT called function objects I tensorflow core grappler optimizers meta optimizer cc Optimization results grappler item tf graph I tensorflow core grappler optimizers meta optimizer cc constant folding Graph size nodes edges time ms I tensorflow core grappler optimizers meta optimizer cc layout Graph size nodes edges time ms I tensorflow core grappler optimizers meta optimizer cc constant folding Graph size nodes edges time ms I tensorflow core grappler optimizers meta optimizer cc TensorRTOptimizer Graph size nodes edges time ms I tensorflow core grappler optimizers meta optimizer cc constant folding Graph size nodes edges time ms I tensorflow core grappler optimizers meta optimizer cc Optimization results grappler item TRTEngineOp native segment I tensorflow core grappler optimizers meta optimizer cc constant folding Graph size nodes edges time ms I tensorflow core grappler optimizers meta optimizer cc layout Graph size nodes edges time ms I tensorflow core grappler optimizers meta optimizer cc constant folding Graph size nodes edges time ms I tensorflow core grappler optimizers meta optimizer cc TensorRTOptimizer Graph size nodes edges time ms I tensorflow core grappler optimizers meta optimizer cc constant folding Graph size nodes edges time ms I tensorflow stream executor cuda cuda gpu executor cc successful NUMA node read SysFS negative value must Since TRT model nodes one TRT Engine node make sense convert via UFF Would get speed improvement Or bug latest version TF docker Thanks Describe expected behavior Code reproduce issue Provide reproducible test case bare minimum necessary generate problem Other info logs Include logs source code would helpful diagnose problem If including tracebacks please include full traceback Large logs files attached 