This PR allows add tensorflow dependency Go API using rules go bazel I trying fine tune inceptionv model using slim tensorflow library I tried read source code proper documentation figured things I able fine tune save checkpoint However I unable understand certain things writing code Here steps I followed I created tf record training data fine I reading data using code import tensorflow tf import tensorflow contrib slim nets nets import tensorflow contrib slim slim import matplotlib pyplot plt import numpy np get data labels data path home sfarkya nvidia challenge datasets detrac train tfrecords Training setting num epochs initial learning rate learning rate decay factor num epochs decay num classes load checkpoint model path home sfarkya nvidia challenge datasets detrac inception v ckpt log directory log dir home sfarkya nvidia challenge datasets detrac fine tuned model tf Session sess feature { train image tf FixedLenFeature tf string train label tf FixedLenFeature tf int } Create list filenames pass queue filename queue tf train string input producer data path num epochs Define reader read next record reader tf TFRecordReader serialized example reader read filename queue Decode record read reader features tf parse single example serialized example features feature Convert image data string back numbers image tf decode raw features train image tf float Cast label data int label tf cast features train label tf int Reshape image data original shape image tf reshape image Creates batches randomly shuffling tensors images labels tf train shuffle batch image label batch size capacity num threads min dequeue url Now I finetuning model using slim code init op tf group tf global variables initializer tf local variables initializer sess run init op Create coordinator run QueueRunner objects coord tf train Coordinator threads tf train start queue runners coord coord load model load inception model slim library using inception v inputL tf placeholder tf float img lbl sess run images labels one hot labels slim one hot encoding lbl num classes slim arg scope slim nets inception inception v arg scope logits inceptionv nets inception inception v inputs img num classes training True dropout keep prob Restore convolutional layers variables restore slim get variables restore exclude InceptionV Logits InceptionV AuxLogits init fn slim assign checkpoint fn model path variables restore loss function loss tf losses softmax cross entropy onehot labels one hot labels logits logits total loss tf losses get total loss train operation train op slim learning create train op total loss + loss optimizer tf train AdamOptimizer learning rate e print Im Start training slim learning train train op log dir init fn init fn save interval secs number steps Now I questions code I quite unable figure Once code reaches slim learning train I see anything printing however training I see log Now How I give number epochs code Right running step step step batch size How I make sure code tf train shuffle batch I repeating images I training whole dataset How I print loss values training If I create validation set I switch betweem training model validation Thanks help System information Have I written custom code opposed using stock example script provided TensorFlow yes OS Platform Distribution e g Linux Ubuntu Linux amd SMP Debian + deb u x GNU Linux TensorFlow installed source binary binary TensorFlow version use command rc Python version Bazel version compiling source N A GCC Compiler version compiling source N A CUDA cuDNN version N A GPU model memory Google Cloud TPU v running TF Exact command reproduce See Describe problem Using Keras LSTM layer processing Google Cloud TPU results ValueError lstm unrolled The layer behaves expected TPUEstimator configured use CPU lstm loop unrolled either TPU CPU Error raised TPU failure case ValueError Cannot create gradient accumulator tensor TPUReplicate loop lstm Identity inside XLA loop maximum iterations passed tf loop call TPUReplicate loop lstm context Source code logs Test code minimal example keras lstm test py python Test Keras model LSTM TPU Based https github com tensorflow tpu blob master models official resnet future import absolute import future import division future import print function absl import flags import absl logging logging pylint disable unused import import tensorflow tf import numpy np tensorflow contrib tpu python tpu import tpu config tensorflow contrib tpu python tpu import tpu estimator tensorflow contrib tpu python tpu import tpu optimizer Define flags system FLAGS flags FLAGS flags DEFINE bool use tpu True help Use TPU execute model training evaluation If use tpu false use whatever devices available TensorFlow default e g CPU GPU flags DEFINE string tpu name default None help Name Cloud TPU Cluster Resolvers flags DEFINE string model dir default None help The directory model training evaluation summaries stored flags DEFINE bool unroll lstm default False help Unrolls Keras LSTM True def main unused argv Get TPU GRPC URL needed FLAGS use tpu tpu cluster resolver tf contrib cluster resolver TPUClusterResolver FLAGS tpu name tpu grpc url tpu cluster resolver get master else tpu grpc url None Set configuration TPU config tpu config RunConfig master tpu grpc url model dir FLAGS model dir Create TPUEstimator test estimator tpu estimator TPUEstimator use tpu FLAGS use tpu model fn test model fn config config train batch size Train estimator steps test estimator train input fn max steps def input fn params Generate random dataset correct shape data np random rand astype np float label np random rand astype np float Repeat batch rand dataset tf data Dataset tensor slices data label repeat rand dataset rand dataset apply tf contrib data batch drop remainder params batch size Make input fn TPUEstimator train step rand dataset fn rand dataset make one shot iterator get next return rand dataset fn def test model fn features labels mode params The Keras LSTM layer causes error unless unrolled predictions tf keras layers LSTM unroll FLAGS unroll lstm features Create ops TPUEstimatorSpec loss tf losses mean squared error labels labels predictions predictions optimizer tf train GradientDescentOptimizer FLAGS use tpu optimizer tpu optimizer CrossShardOptimizer optimizer global step tf train get global step train op optimizer minimize loss global step Return TPUEstimatorSpec return tpu estimator TPUEstimatorSpec mode mode loss loss train op train op name main Do thing print main wrapper tf logging set verbosity tf logging INFO tf app run Error TPU unroll loop set False sanitized log file $ python keras lstm test py model dir gs bucket keras lstm test tpu name tpu name unroll lstm False WARNING Logging flag parsing goes stderr W tf logging py From usr local lib python dist packages tensorflow contrib learn python learn datasets base py retry tensorflow contrib learn python learn datasets base deprecated removed future version Instructions updating Use retry module similar alternatives main wrapper W init py file cache unavailable using oauth client Traceback recent call last File usr local lib python dist packages googleapiclient discovery cache init py line autodetect import file cache File usr local lib python dist packages googleapiclient discovery cache file cache py line module file cache unavailable using oauth client ImportError file cache unavailable using oauth client I tensorflow core platform cpu feature guard cc Your CPU supports instructions TensorFlow binary compiled use AVX FMA I tensorflow core distributed runtime rpc grpc channel cc Initialize GrpcChannelCache job local { localhost } I tensorflow core distributed runtime rpc grpc server lib cc Started server target grpc localhost W tf logging py Estimator model fn function test model fn x f eca f includes params argument params passed Estimator I tf logging py Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host input training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max tf random seed None task type worker global id cluster chief True cluster spec tensorflow python training server lib ClusterSpec object x f eca cluster None model dir gs bucket keras lstm test num worker replicas task id log step count steps master u grpc save checkpoints steps None keep checkpoint every n hours evaluation master u grpc service None save summary steps num ps replicas } I tf logging py Querying Tensorflow master grpc TPU system metadata W tensorflow core distributed runtime rpc grpc session cc GrpcSession ListDevices initialize session empty graph defaults session yet created I tf logging py Found TPU system I tf logging py Num TPU Cores I tf logging py Num TPU Workers I tf logging py Num TPU Cores Per Worker I tf logging py Available Devices DeviceAttributes job tpu worker replica task device CPU CPU DeviceAttributes job tpu worker replica task device XLA CPU XLA CPU DeviceAttributes job tpu worker replica task device XLA GPU XLA GPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU SYSTEM TPU SYSTEM I tf logging py Calling model fn Traceback recent call last File keras lstm test py line module tf app run File usr local lib python dist packages tensorflow python platform app py line run sys exit main argv File keras lstm test py line main test estimator train input fn max steps File usr local lib python dist packages tensorflow python estimator estimator py line train loss self train model input fn hooks saving listeners File usr local lib python dist packages tensorflow python estimator estimator py line train model features labels model fn lib ModeKeys TRAIN self config File usr local lib python dist packages tensorflow python estimator estimator py line call model fn model fn results self model fn features features kwargs File usr local lib python dist packages tensorflow contrib tpu python tpu tpu estimator py line model fn train tpu system ctx model fn wrapper dequeue fn File usr local lib python dist packages tensorflow contrib tpu python tpu tpu estimator py line train tpu system device assignment ctx device assignment File usr local lib python dist packages tensorflow contrib tpu python tpu tpu py line shard name name File usr local lib python dist packages tensorflow contrib tpu python tpu tpu py line replicate outputs computation computation inputs File usr local lib python dist packages tensorflow contrib tpu python tpu tpu estimator py line multi tpu train steps single shard name b loop File usr local lib python dist packages tensorflow contrib tpu python tpu training loop py line repeat cond body wrapper inputs inputs infeed queue infeed queue name name File usr local lib python dist packages tensorflow contrib tpu python tpu training loop py line loop name name File usr local lib python dist packages tensorflow python ops control flow ops py line loop result loop context BuildLoop cond body loop vars shape invariants File usr local lib python dist packages tensorflow python ops control flow ops py line BuildLoop pred body original loop vars loop vars shape invariants File usr local lib python dist packages tensorflow python ops control flow ops py line BuildLoop body result body packed vars body File usr local lib python dist packages tensorflow contrib tpu python tpu training loop py line body wrapper outputs body inputs + dequeue ops File usr local lib python dist packages tensorflow contrib tpu python tpu training loop py line body wrapper return + + convert list body args File usr local lib python dist packages tensorflow contrib tpu python tpu tpu estimator py line train step self call model fn features labels File usr local lib python dist packages tensorflow contrib tpu python tpu tpu estimator py line call model fn estimator spec self model fn features features kwargs File keras lstm test py line test model fn train op optimizer minimize loss global step File usr local lib python dist packages tensorflow python training optimizer py line minimize grad loss grad loss File usr local lib python dist packages tensorflow contrib tpu python tpu tpu optimizer py line compute gradients return self opt compute gradients loss var list var list kwargs File usr local lib python dist packages tensorflow python training optimizer py line compute gradients colocate gradients ops colocate gradients ops File usr local lib python dist packages tensorflow python ops gradients impl py line gradients gate gradients aggregation method stop gradients File usr local lib python dist packages tensorflow python ops gradients impl py line GradientsHelper lambda grad fn op grads File usr local lib python dist packages tensorflow python ops gradients impl py line MaybeCompile return grad fn Exit early File usr local lib python dist packages tensorflow python ops gradients impl py line lambda lambda grad fn op grads File usr local lib python dist packages tensorflow python ops tensor array grad py line TensorArrayWriteGrad grad g read index File usr local lib python dist packages tensorflow python ops tensor array ops py line read return self implementation read index name name File usr local lib python dist packages tensorflow python ops tensor array ops py line read name name File usr local lib python dist packages tensorflow python ops gen data flow ops py line tensor array read v dtype dtype name name File usr local lib python dist packages tensorflow python framework op def library py line apply op helper op def op def File usr local lib python dist packages tensorflow python framework ops py line create op op def op def File usr local lib python dist packages tensorflow python framework ops py line init self control flow post processing File usr local lib python dist packages tensorflow python framework ops py line control flow post processing self control flow context AddOp self File usr local lib python dist packages tensorflow python ops control flow ops py line AddOp self AddOpInternal op File usr local lib python dist packages tensorflow python ops control flow ops py line AddOpInternal real x self AddValue x File usr local lib python dist packages tensorflow python ops control flow ops py line AddValue real val grad ctxt grad state GetRealValue val File usr local lib python dist packages tensorflow python ops control flow ops py line GetRealValue history value cur grad state AddForwardAccumulator cur value File usr local lib python dist packages tensorflow python ops control flow ops py line AddForwardAccumulator value self forward context File usr local lib python dist packages tensorflow python ops control flow ops py line GetMaxSizeFromNestedMaximumIterations tf loop call % % value name ctxt name ValueError Cannot create gradient accumulator tensor TPUReplicate loop lstm Identity inside XLA loop maximum iterations passed tf loop call TPUReplicate loop lstm context Working expected TPU unroll lstm set True $ python keras lstm test py model dir gs bucket keras lstm test tpu name tpu name unroll lstm True WARNING Logging flag parsing goes stderr W tf logging py From usr local lib python dist packages tensorflow contrib learn python learn datasets base py retry tensorflow contrib learn python learn datasets base deprecated removed future version Instructions updating Use retry module similar alternatives main wrapper W init py file cache unavailable using oauth client Traceback recent call last File usr local lib python dist packages googleapiclient discovery cache init py line autodetect import file cache File usr local lib python dist packages googleapiclient discovery cache file cache py line module file cache unavailable using oauth client ImportError file cache unavailable using oauth client I tensorflow core platform cpu feature guard cc Your CPU supports instructions TensorFlow binary compiled use AVX FMA I tensorflow core distributed runtime rpc grpc channel cc Initialize GrpcChannelCache job local { localhost } I tensorflow core distributed runtime rpc grpc server lib cc Started server target grpc localhost W tf logging py Estimator model fn function test model fn x ffb dff includes params argument params passed Estimator I tf logging py Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host input training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max tf random seed None task type worker global id cluster chief True cluster spec tensorflow python training server lib ClusterSpec object x ffb dfdcd cluster None model dir gs bucket keras lstm test num worker replicas task id log step count steps master u grpc save checkpoints steps None keep checkpoint every n hours evaluation master u grpc service None save summary steps num ps replicas } I tf logging py Querying Tensorflow master grpc TPU system metadata W tensorflow core distributed runtime rpc grpc session cc GrpcSession ListDevices initialize session empty graph defaults session yet created I tf logging py Found TPU system I tf logging py Num TPU Cores I tf logging py Num TPU Workers I tf logging py Num TPU Cores Per Worker I tf logging py Available Devices DeviceAttributes job tpu worker replica task device CPU CPU DeviceAttributes job tpu worker replica task device XLA CPU XLA CPU DeviceAttributes job tpu worker replica task device XLA GPU XLA GPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU TPU DeviceAttributes job tpu worker replica task device TPU SYSTEM TPU SYSTEM I tf logging py Calling model fn I tf logging py Done calling model fn I tf logging py Create CheckpointSaverHook I tf logging py TPU job name tpu worker I tf logging py Graph finalized I tf logging py Running local init op I tf logging py Done running local init op I tf logging py Init TPU system I tf logging py Start infeed thread controller I tf logging py Starting infeed thread controller I tf logging py Start outfeed thread controller I tf logging py Starting outfeed thread controller I tf logging py Enqueue next batch es data infeed I tf logging py Dequeue next batch es data outfeed I tf logging py Saving checkpoints gs bucket keras lstm test model ckpt I tf logging py loss step I tf logging py loss step I tf logging py Enqueue next batch es data infeed I tf logging py Dequeue next batch es data outfeed I tf logging py Enqueue next batch es data infeed I tf logging py Dequeue next batch es data outfeed I tf logging py Enqueue next batch es data infeed I tf logging py Dequeue next batch es data outfeed I tf logging py Enqueue next batch es data infeed I tf logging py Dequeue next batch es data outfeed I tf logging py Saving checkpoints gs bucket keras lstm test model ckpt I tf logging py Stop infeed thread controller I tf logging py Shutting InfeedController thread I tf logging py InfeedController received shutdown signal stopping I tf logging py Infeed thread finished shutting I tf logging py Stop output thread controller I tf logging py Shutting OutfeedController thread I tf logging py OutfeedController received shutdown signal stopping I tf logging py Outfeed thread finished shutting I tf logging py Shutdown TPU system I tf logging py Loss final step Working expected unroll loop set either True False CPU $ python keras lstm test py model dir gs bucket keras lstm test use tpu False unroll lstm False WARNING Logging flag parsing goes stderr W tf logging py From usr local lib python dist packages tensorflow contrib learn python learn datasets base py retry tensorflow contrib learn python learn datasets base deprecated removed future version Instructions updating Use retry module similar alternatives main wrapper W tf logging py Estimator model fn function test model fn x f ace f includes params argument params passed Estimator I tf logging py Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host input training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max tf random seed None task type worker global id cluster chief True cluster spec tensorflow python training server lib ClusterSpec object x f ace cluster None model dir gs bucket keras lstm test num worker replicas task id log step count steps master save checkpoints steps None keep checkpoint every n hours evaluation master service None save summary steps num ps replicas } I tf logging py Calling model fn I tf logging py Running train CPU I tf logging py Done calling model fn I tf logging py Create CheckpointSaverHook I tf logging py Graph finalized I tensorflow core platform cpu feature guard cc Your CPU supports instructions TensorFlow binary compiled use AVX FMA I tf logging py Running local init op I tf logging py Done running local init op I tf logging py Saving checkpoints gs bucket keras lstm test model ckpt I tf logging py loss step I tf logging py Saving checkpoints gs bucket keras lstm test model ckpt I tf logging py Loss final step $ python keras lstm test py model dir gs bucket keras lstm test use tpu False unroll lstm True WARNING Logging flag parsing goes stderr W tf logging py From usr local lib python dist packages tensorflow contrib learn python learn datasets base py retry tensorflow contrib learn python learn datasets base deprecated removed future version Instructions updating Use retry module similar alternatives main wrapper W tf logging py Estimator model fn function test model fn x fb e includes params argument params passed Estimator I tf logging py Using config { tpu config TPUConfig iterations per loop num shards None computation shape None per host input training True tpu job name None initial infeed sleep secs None save checkpoints secs session config None keep checkpoint max tf random seed None task type worker global id cluster chief True cluster spec tensorflow python training server lib ClusterSpec object x fb c cluster None model dir gs bucket keras lstm test num worker replicas task id log step count steps master save checkpoints steps None keep checkpoint every n hours evaluation master service None save summary steps num ps replicas } I tf logging py Calling model fn I tf logging py Running train CPU I tf logging py Done calling model fn I tf logging py Create CheckpointSaverHook I tf logging py Graph finalized I tensorflow core platform cpu feature guard cc Your CPU supports instructions TensorFlow binary compiled use AVX FMA I tf logging py Running local init op I tf logging py Done running local init op I tf logging py Saving checkpoints gs bucket keras lstm test model ckpt I tf logging py loss step I tf logging py Saving checkpoints gs bucket keras lstm test model ckpt I tf logging py Loss final step 